{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM60M+ANa6HtmGncfOKSQER"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup\n","- 참고문서\n","  - https://wikidocs.net/book/14473\n","  - https://wikidocs.net/book/14314\n","  - https://github.com/teddylee777/langchain-kr"],"metadata":{"id":"69J9opGOP5ol"}},{"cell_type":"markdown","source":["## Install"],"metadata":{"id":"lqdqeq5sQS9Y"}},{"cell_type":"code","source":["!pip install -U langchain langchain-community langchain-experimental langchain-core langchain-openai langsmith"],"metadata":{"id":"TaNWRKfJyGmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## OpenAI Key 등록\n","- [OpenAI Key 발급방법](https://teddylee777.github.io/openai/openai-api-key/)\n","- [OpenAI 요금](https://openai.com/api/pricing/)"],"metadata":{"id":"VMjiMrgcQVac"}},{"cell_type":"code","source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = '생성한 키 입력'"],"metadata":{"id":"8Y5OFyB3QVka"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompt\n","- 프롬프트는 사용자와 언어 모델 간의 대화에서 질문이나 요청의 형태로 제시되는 입력문입니다. 이는 모델이 어떤 유형의 응답을 제공할지 결정하는 데 중요한 역할을 합니다."],"metadata":{"id":"AWzQ1wDrQznD"}},{"cell_type":"markdown","source":["## 프롬프트 작성방법"],"metadata":{"id":"fTdXL4oSRBn8"}},{"cell_type":"markdown","source":["1. 명확성과 구체성\n","  - 질문은 명확하고 구체적이어야 합니다. 모호한 질문은 LLM 모델의 혼란을 초래할 수 있기 때문입니다.\n","  - 예시: \"다음 주 주식 시장에 영향을 줄 수 있는 예정된 이벤트들은 무엇일까요?\"는 \"주식 시장에 대해 알려주세요.\"보다 더 구체적이고 명확한 질문입니다.\n","2. 배경 정보를 포함\n","  - 모델이 문맥을 이해할 수 있도록 필요한 배경 정보를 제공하는 것이 좋습니다. 이는 환각 현상(hallucination)이 발생할 위험을 낮추고, 관련성 높은 응답을 생성하는 데 도움을 줍니다.\n","  - 예시: \"2020년 미국 대선의 결과를 바탕으로 현재 정치 상황에 대한 분석을 해주세요.\"\n","3. 간결함\n","  - 핵심 정보에 초점을 맞추고, 불필요한 정보는 배제합니다. 프롬프트가 길어지면 모델이 덜 중요한 부분에 집중하거나 상당한 영향을 받는 문제가 발생할 수 있습니다.\n","  - 예시: \"2021년에 발표된 삼성전자의 ESG 보고서를 요약해주세요.\"\n","4. 열린 질문 사용\n","  - 열린 질문을 통해 모델이 자세하고 풍부한 답변을 제공하도록 유도합니다. 단순한 '예' 또는 '아니오'로 대답할 수 있는 질문보다는 더 많은 정보를 제공하는 질문이 좋습니다.\n","  - 예시: \"신재생에너지에 대한 최신 연구 동향은 무엇인가요?\"\n","5. 명확한 목표 설정\n","  - 얻고자 하는 정보나 결과의 유형을 정확하게 정의합니다. 이는 모델이 명확한 지침에 따라 응답을 생성하도록 돕습니다.\n","  - 예시: \"AI 윤리에 대한 문제점과 해결 방안을 요약하여 설명해주세요.\"\n","6. 언어와 문체\n","  - 대화의 맥락에 적합한 언어와 문체를 선택합니다. 이는 모델이 상황에 맞는 표현을 선택하는데 도움이 됩니다.\n","  - 예시: 공식적인 보고서를 요청하는 경우, \"XX 보고서에 대한 전문적인 요약을 부탁드립니다.\"와 같이 정중한 문체를 사용합니다."],"metadata":{"id":"4Rp1xwDpRIA2"}},{"cell_type":"markdown","source":["## 프롬프트 템플릿\n","- PromptTemplate은 단일 문장 또는 간단한 명령을 입력하여 단일 문장 또는 간단한 응답을 생성하는 데 사용되는 프롬프트를 구성할 수 있는 문자열 템플릿입니다.\n","- Python의 문자열 포맷팅을 사용하여 동적으로 특정한 위치에 입력 값을 포함시킬 수 있습니다."],"metadata":{"id":"nX9xSdQZShFB"}},{"cell_type":"markdown","source":["### 문자열 템플릿"],"metadata":{"id":"NV-m3NhgS8VM"}},{"cell_type":"markdown","source":["다음 예제는 `langchain_core.prompts` 모듈의 `PromptTemplate` 클래스를 사용하여, 'name'과 'age'라는 두 개의 변수를 포함하는 프롬프트 템플릿을 정의하고 있습니다. 이 템플릿을 이용하여 실제 입력값을 해당 위치에 채워 넣어 완성된 프롬프트를 생성하는 과정을 보여줍니다.\n","1. `PromptTemplate.from_template` 메서드를 사용하여 문자열 템플릿으로부터 `PromptTemplate` 인스턴스를 생성합니다. 이때, `template_text` 변수에 정의된 템플릿 문자열이 사용됩니다.\n","2. 생성된 `PromptTemplate` 인스턴스의 `format` 메서드를 사용하여, 실제 'name'과 'age' 값으로 템플릿에 채워서 프롬프트를 구성합니다. 여기서는 `name=\"홍길동\", age=30`으로 지정하여 호출합니다.\n","3. 결과적으로, `filled_prompt` 변수에는 \"안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.\"라는 완성된 프롬프트 문자열이 저장됩니다."],"metadata":{"id":"NkZHaYVSTDfH"}},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","\n","# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의\n","template_text = \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\"\n","\n","template_text"],"metadata":{"id":"C7cMad8mQtV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PromptTemplate 인스턴스를 생성\n","prompt_template = PromptTemplate.from_template(template_text)\n","\n","prompt_template"],"metadata":{"id":"XaFJOqXvQtYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 템플릿에 값을 채워서 프롬프트를 완성\n","filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n","\n","filled_prompt"],"metadata":{"id":"xKq7jaPbQtb0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 프롬프트 템플릿 결합"],"metadata":{"id":"GCK-ZG4RTjba"}},{"cell_type":"markdown","source":["`PromptTemplate` 클래스는 문자열을 기반으로 프롬프트 템플릿을 생성하고, + 연산자를 사용하여 직접 결합하는 동작을 지원합니다. `PromptTemplate` 인스턴스 간의 직접적인 결합뿐만 아니라, 이들 인스턴스와 문자열로 이루어진 템플릿을 결합하여 새로운 `PromptTemplate` 인스턴스를 생성하는 것도 가능합니다.\n","- 문자열 + 문자열\n","- PromptTemplate + PromptTemplate\n","- PromptTemplate + 문자열"],"metadata":{"id":"rSU1sLahWu4a"}},{"cell_type":"code","source":["# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n","combined_prompt = (\n","              prompt_template\n","              + PromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를 수 없습니다.\")\n","              + \"\\n\\n{language}로 번역해주세요.\"\n",")\n","\n","combined_prompt"],"metadata":{"id":"NT-f6V6LQte_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- `format` 메소드를 사용하여 앞에서 생성한 템플릿의 매개변수에 입력 값을 지정합니다.\n","- LLM에게 전달할 프롬프트가 완성되는데, 주어진 문장을 \"영어로 번역해주세요.\" 라는 지시사항을 포함하고 있습니다."],"metadata":{"id":"calMtYlpXM1u"}},{"cell_type":"code","source":["combined_prompt.format(name=\"홍길동\", age=30, language=\"영어\")"],"metadata":{"id":"BiI6TshzQtkd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- ChatOpenAI 인스턴스를 생성하여 프롬프트 텍스트를 전달하고, 모델의 출력을 StrOutputParser를 통해 문자열로 변환하는 LLM 체인을 구성합니다.\n","- invoke 메소드를 사용하여 파이프라인을 실행하고, 최종적으로 문자열 출력을 얻습니다.\n","- 모델의 응답은 프롬프트에 주어진 문장을 영어로 번역한 텍스트가 출력됩니다."],"metadata":{"id":"TMx5_fooYvf7"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# 모델 선언\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n","# LLM 체인 선언\n","chain = combined_prompt | llm | StrOutputParser()\n","# 실행\n","chain.invoke({\"age\":30, \"language\":\"영어\", \"name\":\"홍길동\"})"],"metadata":{"id":"4LcZAC6pQtnD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 챗 프롬프트 템플릿\n","- ChatPromptTemplate은 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는 데 사용됩니다.\n","- 이는 대화형 모델이나 챗봇 개발에 주로 사용됩니다.\n","- 입력은 여러 메시지를 원소로 갖는 리스트로 구성되며, 각 메시지는 역할(role)과 내용(content)으로 구성됩니다."],"metadata":{"id":"MFbqhGLpZJGK"}},{"cell_type":"markdown","source":["### Message 유형"],"metadata":{"id":"oO4iRBFFZOfZ"}},{"cell_type":"markdown","source":["- SystemMessage: 시스템의 기능을 설명합니다.\n","- HumanMessage: 사용자의 질문을 나타냅니다.\n","- AIMessage: AI 모델의 응답을 제공합니다.\n","- FunctionMessage: 특정 함수 호출의 결과를 나타냅니다.\n","- ToolMessage: 도구 호출의 결과를 나타냅니다."],"metadata":{"id":"x1yQRYT_ZpkQ"}},{"cell_type":"markdown","source":["### 튜플 형태의 메시지 리스트"],"metadata":{"id":"dveDQ43AZ1As"}},{"cell_type":"markdown","source":["- `ChatPromptTemplate.from_messages` 메서드를 사용하여 메시지 리스트로부터 `ChatPromptTemplate` 인스턴스를 생성하는 방식은 대화형 프롬프트를 생성하는 데 유용합니다.\n","- 이 메서드는 2-튜플 형태의 메시지 리스트를 입력 받아, 각 `메시지의 역할(type)`과 `내용(content)`을 기반으로 프롬프트를 구성합니다.\n","\n","다음 예시에서 `ChatPromptTemplate.from_messages` 메서드는 전달된 메시지들을 기반으로 프롬프트를 구성합니다. 그리고 `format_messages` 메서드는 사용자의 입력을 프롬프트에 동적으로 삽입하여, 최종적으로 대화형 상황을 반영한 메시지 리스트를 생성합니다. 시스템은 자신의 기능을 설명하고, 사용자는 천문학 관련 질문을 합니다."],"metadata":{"id":"eLDWd677Z1KP"}},{"cell_type":"code","source":["# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n","\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n","    (\"user\", \"{user_input}\"),\n","])\n","\n","messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n","messages"],"metadata":{"id":"D1I4MdGZQtqa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- chat_prompt, llm, StrOutputParser()를 순차적인 파이프라인으로 연결하여 구성된 chain을 사용합니다.\n","- invoke 메소드를 호출하면 사용자 입력을 받아 언어 모델에 전달하고, 모델의 응답을 처리하여 최종 문자열 결과를 반환하는 과정을 자동화하여 수행합니다.\n","- 사용자는 천문학 관련 질문에 대한 언어 모델의 응답을 얻을 수 있습니다."],"metadata":{"id":"1eRR6zL4aSXP"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","\n","chain = chat_prompt | llm | StrOutputParser()\n","\n","chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"],"metadata":{"id":"KS16XoVtaSf9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MessagePromptTemplate 활용"],"metadata":{"id":"azHDbXENbBF7"}},{"cell_type":"markdown","source":["- 다음 예제는 `SystemMessagePromptTemplate`와 `HumanMessagePromptTemplate`를 사용하여 천문학 질문에 답변할 수 있는 시스템에 대한 대화형 프롬프트를 생성합니다.\n","- `ChatPromptTemplate.from_messages` 메소드를 통해 시스템 메시지와 사용자 메시지 템플릿을 포함하는 챗 프롬프트를 구성합니다.\n","- 이후, `chat_prompt.format_messages` 메서드를 사용하여 사용자의 질문을 포함한 메시지 리스트를 동적으로 생성합니다."],"metadata":{"id":"jYpfTBKlbDBx"}},{"cell_type":"code","source":["# MessagePromptTemplate 활용\n","\n","from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate\n","\n","chat_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n","        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n","    ]\n",")\n","\n","messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n","messages"],"metadata":{"id":"l60VQIZtbCFv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 이렇게 생성된 메시지 리스트는 대화형 인터페이스나 언어 모델과의 상호작용을 위한 입력으로 사용될 수 있습니다.\n","- 각 메시지는 role (메시지를 말하는 주체, 여기서는 system 또는 user)과 content (메시지의 내용) 속성을 포함합니다.\n","- 이 구조는 시스템과 사용자 간의 대화 흐름을 명확하게 표현하며, 언어 모델이 이를 기반으로 적절한 응답을 생성할 수 있도록 돕습니다."],"metadata":{"id":"zMdV0omKbWM9"}},{"cell_type":"code","source":["chain = chat_prompt | llm | StrOutputParser()\n","\n","chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"],"metadata":{"id":"M_3-0toPbSeG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model\n","- 랭체인 문서에 따르면 `LLM`과 `Chat Model` 클래스는 각각 다른 형태의 입력과 출력을 다루는 언어 모델을 나타냅니다.\n","- 이 두 모델은 각기 다른 특성과 용도를 가지고 있어, 사용자의 요구사항에 맞게 선택하여 사용할 수 있습니다.\n","- 일반적으로 `LLM`은 주로 단일 요청에 대한 복잡한 출력을 생성하는 데 적합한 반면, `Chat Model`은 사용자와의 상호작용을 통한 연속적인 대화 관리에 더 적합합니다."],"metadata":{"id":"NbD19_orbh8D"}},{"cell_type":"markdown","source":["## Large Language Models (LLMs)"],"metadata":{"id":"cI8_wV0Hc7oB"}},{"cell_type":"markdown","source":["- `기능`\n","  - LLM 클래스는 텍스트 문자열을 입력으로 받아 처리한 후, 텍스트 문자열을 반환합니다. 이 모델은 광범위한 언어 이해 및 텍스트 생성 작업에 사용됩니다. 예를 들어, 문서 요약, 콘텐츠 생성, 질문에 대한 답변 생성 등 복잡한 자연어 처리 작업을 수행할 수 있습니다.\n","- `예시`\n","  - 사용자가 특정 주제에 대한 설명을 요청할 때, LLM은 주어진 텍스트 입력을 바탕으로 상세한 설명을 생성하여 반환할 수 있습니다."],"metadata":{"id":"K9PoPzngdBUR"}},{"cell_type":"markdown","source":["### LLM 인터페이스 특징"],"metadata":{"id":"o2mek3FYd6oC"}},{"cell_type":"markdown","source":["- `표준화된 인터페이스`\n","  - 랭체인의 LLM 클래스는 사용자가 문자열을 입력으로 제공하면, 그에 대한 응답으로 문자열을 반환하는 표준화된 방식을 제공합니다.\n","  - 이는 다양한 LLM 제공 업체 간의 호환성을 보장하며, 사용자는 복잡한 API 변환 작업 없이 여러 LLM을 쉽게 탐색하고 사용할 수 있습니다.\n","- `다양한 LLM 제공 업체 지원`\n","  - 랭체인은 OpenAI의 GPT 시리즈, Cohere의 LLM, Hugging Face의 Transformer 모델 등 다양한 LLM 제공 업체와의 통합을 지원합니다.\n","  - 이를 통해 사용자는 자신의 요구 사항에 가장 적합한 모델을 선택하여 사용할 수 있습니다."],"metadata":{"id":"R4XvcxM4d_fa"}},{"cell_type":"markdown","source":["### 예제\n","- 다음 예제는 LLM을 사용하여 직접적인 질문에 대한 답변을 생성하는 방법을 보여줍니다.\n","- langchain_openai 모듈의 OpenAI 클래스를 사용하여 LLM 인스턴스를 생성합니다.\n","- invoke 메소드를 사용하여 질문(\"한국의 대표적인 관광지 3군데를 추천해주세요.\")을 LLM에 전달하고, 관광지 추천을 받습니다."],"metadata":{"id":"wJAlkOQieYd3"}},{"cell_type":"code","source":["from langchain_openai import OpenAI\n","\n","llm = OpenAI()\n","\n","llm.invoke(\"한국의 대표적인 관광지 3군데를 추천해주세요.\")"],"metadata":{"id":"8A2UXyqnbShH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Chat Model"],"metadata":{"id":"Y9-CrMYfdFLh"}},{"cell_type":"markdown","source":["- `기능`\n","  - Chat Model 클래스는 메시지의 리스트를 입력으로 받고, 하나의 메시지를 반환합니다.\n","  - 이 모델은 대화형 상황에 최적화되어 있으며, 사용자와의 연속적인 대화를 처리하는 데 사용됩니다.\n","  - Chat Model은 대화의 맥락을 유지하면서 적절한 응답을 생성하는 데 중점을 둡니다.\n","- `예시`\n","  - 사용자가 챗봇과 대화하는 상황에서, 사용자의 질문과 이전 대화 내용을 고려하여 적절한 답변을 생성합니다."],"metadata":{"id":"Pnu_7K_mdIAe"}},{"cell_type":"markdown","source":["### Chat Model 인터페이스 특징"],"metadata":{"id":"2IzPzo_Meu2P"}},{"cell_type":"markdown","source":["- `대화형 입력과 출력`\n","  - Chat Model은 대화의 연속성을 고려하여 입력된 메시지 리스트를 기반으로 적절한 응답 메시지를 생성합니다.\n","  - 챗봇, 가상 비서, 고객 지원 시스템 등 대화 기반 서비스에 어울립니다.\n","- `다양한 모델 제공 업체와의 통합`\n","  - 랭체인은 OpenAI, Cohere, Hugging Face 등 다양한 모델 제공 업체와의 통합을 지원합니다.\n","  - 이를 통해 개발자는 여러 소스의 Chat Models를 조합하여 활용할 수 있습니다.\n","- `다양한 작동 모드 지원`\n","  - 랭체인은 동기(sync), 비동기(async), 배치(batching), 스트리밍(streaming) 모드에서 모델을 사용할 수 있는 기능을 제공합니다.\n","  - 다양한 애플리케이션 요구사항과 트래픽 패턴에 따라 유연한 대응이 가능합니다."],"metadata":{"id":"Q4yMmJ7Fe0nM"}},{"cell_type":"markdown","source":["### 예제\n","- 다음 예제는 Chat Model을 사용하여 대화형 인터페이스를 구현하는 방법을 나타냅니다.\n","- ChatPromptTemplate를 사용하여 대화형 프롬프트를 생성합니다.\n","- 이 프롬프트는 시스템이 여행 전문가라는 정보와 사용자 입력을 포함합니다.\n","- ChatOpenAI 클래스를 사용하여 Chat Model 인스턴스를 생성합니다.\n","- 파이프(|) 연산자를 사용하여 구성요소를 연결하여 체인을 정의합니다.\n","- invoke 메소드를 사용하여 사용자의 질문(\"안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요.\")을 모델에 전달하고 응답을 받습니다."],"metadata":{"id":"tMXykvCMfEew"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","chat = ChatOpenAI()\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"이 시스템은 여행 전문가입니다.\"),\n","    (\"user\", \"{user_input}\"),\n","])\n","\n","chain = chat_prompt | chat\n","chain.invoke({\"user_input\": \"안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요.\"})"],"metadata":{"id":"STGdtor5bSkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LLM 파라미터 설정\n","- LLM 모델의 기본 속성값을 조정하는 방법에 대해서 살펴봅니다.\n","- 모델의 속성에 해당하는 모델 파라미터는 LLM의 출력을 조정하고 최적화하는데 사용되며, 모델이 생성하는 텍스트의 스타일, 길이, 정확도 등에 영향을 주게 됩니다.\n","- 사용하는 모델이나 플랫폼에 따라 세부 내용은 차이가 있습니다."],"metadata":{"id":"_A8eJOnmfX-B"}},{"cell_type":"markdown","source":["일반적으로 적용되는 주요 파라미터는 다음과 같습니다.\n","- `Temperature`\n","  - 생성된 텍스트의 다양성을 조정합니다. 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크면 다양하고 예측하기 어려운 출력을 생성합니다.\n","- `Max Tokens (최대 토큰 수)`\n","  - 생성할 최대 토큰 수를 지정합니다. 생성할 텍스트의 길이를 제한합니다.\n","- `Top P (Top Probability)`\n","  - 생성 과정에서 특정 확률 분포 내에서 상위 P% 토큰만을 고려하는 방식입니다. 이는 출력의 다양성을 조정하는 데 도움이 됩니다.\n","- `Frequency Penalty (빈도 패널티)`\n","  - 값이 클수록 이미 등장한 단어나 구절이 다시 등장할 확률을 감소시킵니다. 이를 통해 반복을 줄이고 텍스트의 다양성을 증가시킬 수 있습니다. (0~1)\n","- `Presence Penalty (존재 패널티)`\n","  - 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률을 조정합니다. 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어의 사용이 장려됩니다. (0~1)\n","- `Stop Sequences (정지 시퀀스)`\n","  - 특정 단어나 구절이 등장할 경우 생성을 멈추도록 설정합니다. 이는 출력을 특정 포인트에서 종료하고자 할 때 사용됩니다."],"metadata":{"id":"_r46ehaTfnJt"}},{"cell_type":"markdown","source":["### 모델 생성할 때, 적용"],"metadata":{"id":"f3gEzFVugnKu"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","# 모델 파라미터 설정\n","params = {\n","    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n","    \"max_tokens\": 100,          # 생성할 최대 토큰 수\n","}\n","\n","kwargs = {\n","    \"frequency_penalty\": 0.5,   # 이미 등장한 단어의 재등장 확률\n","    \"presence_penalty\": 0.5,    # 새로운 단어의 도입을 장려\n","    \"stop\": [\"\\n\"]              # 정지 시퀀스 설정\n","\n","}\n","\n","# 모델 인스턴스를 생성할 때 설정\n","model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", **params, model_kwargs = kwargs)\n","\n","\n","# 모델 호출\n","question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n","response = model.invoke(input=question)\n","\n","# 전체 응답 출력\n","print(response)"],"metadata":{"id":"hikH7f_zbSnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 모델 호출할 때, 적용"],"metadata":{"id":"KqYqiebahKt-"}},{"cell_type":"code","source":["# 모델 파라미터 설정\n","params = {\n","    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n","    \"max_tokens\": 10,          # 생성할 최대 토큰 수\n","}\n","\n","# 모델 인스턴스를 호출할 때 전달\n","response = model.invoke(input=question, **params)\n","\n","# 문자열 출력\n","print(response.content)"],"metadata":{"id":"HiBriBeNbSqD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### bind 메소드\n","- bind 메소드를 사용하여 모델 인스턴스에 파라미터를 추가로 제공할 수 있습니다.\n","- bind 메서드를 사용하는 방식의 장점은 특정 모델 설정을 기본값으로 사용하고자 할 때 유용하며, 특수한 상황에서 일부 파라미터를 다르게 적용하고 싶을 때 사용합니다.\n","- 기본적으로 일관된 파라미터 설정을 유지하면서 상황에 맞춰 유연한 대응이 가능합니다.\n","- 이를 통해 코드의 가독성과 재사용성을 높일 수 있습니다."],"metadata":{"id":"Kv_EIGf9hb8e"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n","    (\"user\", \"{user_input}\"),\n","])\n","\n","model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=100)\n","\n","messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n","\n","before_answer = model.invoke(messages)\n","\n","# # binding 이전 출력\n","print(before_answer)\n","\n","# 모델 호출 시 추가적인 인수를 전달하기 위해 bind 메서드 사용 (응답의 최대 길이를 10 토큰으로 제한)\n","chain = prompt | model.bind(max_tokens=10)\n","\n","after_answer = chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})\n","\n","# binding 이후 출력\n","print(after_answer)"],"metadata":{"id":"nuERvb_QbStr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Output Parser"],"metadata":{"id":"mXebzR1X0zEb"}},{"cell_type":"markdown","source":["- 랭체인에서 출력 파서 (Output Parser) 는 모델의 출력을 처리하고, 그 결과를 원하는 형식으로 변환하는 역할을 합니다.\n","- 출력 파서는 모델에서 반환된 원시 텍스트를 분석하고, 특정 정보를 추출하거나, 출력을 특정 형식으로 재구성하는 데 사용됩니다."],"metadata":{"id":"B0fmk9l206i-"}},{"cell_type":"markdown","source":["## Output Parser의 주요 기능"],"metadata":{"id":"7CtWRWHE06mF"}},{"cell_type":"markdown","source":["- `출력 포맷 변경`\n","  - 모델의 출력을 사용자가 원하는 형식으로 변환합니다. 예를 들어, JSON 형식으로 반환된 데이터를 테이블 형식으로 변환할 수 있습니다.\n","- `정보 추출`\n","  - 원시 텍스트 출력에서 필요한 정보(예: 날짜, 이름, 위치 등)를 추출합니다. 이를 통해 복잡한 텍스트 데이터에서 구조화된 정보를 얻을 수 있습니다.\n","- `결과 정제`\n","  - 모델 출력에서 불필요한 정보를 제거하거나, 응답을 더 명확하게 만드는 등의 후처리 작업을 수행합니다.\n","- `조건부 로직 적용`\n","  - 출력 데이터를 기반으로 특정 조건에 따라 다른 처리를 수행합니다. 예를 들어, 모델의 응답에 따라 사용자에게 추가 질문을 하거나, 다른 모델을 호출할 수 있습니다."],"metadata":{"id":"8r9ySE2h06pO"}},{"cell_type":"markdown","source":["## Output Parser의 사용 사례"],"metadata":{"id":"YAyoTtOZ06st"}},{"cell_type":"markdown","source":["- `자연어 처리(NLP) 애플리케이션`\n","  - 질문 답변 시스템에서 정확한 답변만을 추출하여 사용자에게 제공합니다.\n","- `데이터 분석`\n","  - 대량의 텍스트 데이터에서 특정 패턴이나 통계 정보를 추출하여 분석 보고서를 생성합니다.\n","- `챗봇 개발`\n","  - 대화형 모델의 출력을 분석하여 사용자의 의도를 파악하고, 적절한 대화 흐름을 유지합니다.\n","- `콘텐츠 생성`\n","  - 생성된 콘텐츠에서 중요한 정보를 요약하거나, 특정 형식(예: 블로그 포스트, 뉴스 기사)에 맞게 콘텐츠를 재구성합니다."],"metadata":{"id":"ZrvS3lzH2Fb2"}},{"cell_type":"markdown","source":["## CSV Parser - CommaSeparatedListOutputParser"],"metadata":{"id":"mmJaOrJ02Fk6"}},{"cell_type":"markdown","source":["- `CommaSeparatedListOutputParser`는 쉼표로 구분된 항목 목록을 반환할 필요가 있을 때 유용합니다.\n","- 이 출력 파서를 사용하면, 사용자가 입력한 데이터나 요청한 정보를 쉼표로 구분하여 명확하고 간결한 목록 형태로 제공받을 수 있습니다.\n","- 예를 들어, 여러 개의 데이터 포인트, 이름, 항목 또는 다른 종류의 값들을 나열할 때 이를 통해 효과적으로 정보를 정리하고 사용자에게 전달할 수 있습니다.\n","- 이 방법은 정보를 구조화하고, 가독성을 높이며, 특히 데이터를 다루거나 리스트 형태의 결과를 요구하는 경우에 매우 유용합니다.\n","  - `PromptTemplate`를 사용하여 사용자 입력에 따라 동적으로 프롬프트를 생성합니다. 이때, `subject`를 입력 변수로 받고, `format_instructions`를 부분 변수로 설정합니다."],"metadata":{"id":"KpmD_2pG2nS9"}},{"cell_type":"code","source":["from langchain.output_parsers import CommaSeparatedListOutputParser\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI"],"metadata":{"id":"A-Tr5dVK0qHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 콤마로 구분된 리스트 출력 파서 초기화\n","output_parser = CommaSeparatedListOutputParser()"],"metadata":{"id":"8-ovMPO7Hd7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 출력 형식 지침 가져오기\n","format_instructions = output_parser.get_format_instructions()"],"metadata":{"id":"4ZWuFgqMHd95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 프롬프트 템플릿 설정\n","prompt = PromptTemplate(\n","    # 주제에 대한 다섯 가지를 나열하라는 템플릿\n","    template=\"List five {subject}.\\n{format_instructions}\",\n","    input_variables=[\"subject\"],  # 입력 변수로 'subject' 사용\n","    # 부분 변수로 형식 지침 사용\n","    partial_variables={\"format_instructions\": format_instructions},\n",")"],"metadata":{"id":"kfEnjcPiSnqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ChatOpenAI 모델 초기화\n","model = ChatOpenAI(temperature=0)"],"metadata":{"id":"_1fYfPW1Snti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 프롬프트, 모델, 출력 파서를 연결하여 체인 생성\n","chain = prompt | model | output_parser"],"metadata":{"id":"qdepJZzpHeBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.invoke(\n","    {\"subject\": \"대한민국 관광명소\"}\n",")  # \"대한민국 관광명소\"에 대한 체인 호출 실행"],"metadata":{"id":"UbfGdv8_StFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- `chain.stream`을 사용하여 \"대한민국 관광명소\"에 대한 스트림을 반복 처리합니다.\n","- 반복 중 스트림의 결과를 출력합니다."],"metadata":{"id":"5HsR1TRgS40x"}},{"cell_type":"code","source":["# 아이스크림 맛에 대한 스트림을 순회합니다.\n","for s in chain.stream({\"subject\": \"대한민국 관광명소\"}):\n","    print(s)  # 스트림의 내용을 출력합니다."],"metadata":{"id":"zXUrLYEQS0bU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## JSON Parser - JsonOutputParser"],"metadata":{"id":"XVNg77Xr2ohx"}},{"cell_type":"markdown","source":[],"metadata":{"id":"STT65e6D2rF3"}},{"cell_type":"code","source":[],"metadata":{"id":"Rk6F0wPJ0qKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UCQcuzye0qOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D1aHW1Z70qQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2X74CUzw0qTc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pandas Parser - PandasDataFrameOutputParser"],"metadata":{"id":"NVAYtilJTSvY"}},{"cell_type":"code","source":[],"metadata":{"id":"1XVHYQnu0qWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3hxQrXng0qZJ"},"execution_count":null,"outputs":[]}]}