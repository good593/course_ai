from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

import streamlit as st

from .model import get_client_of_openai

@st.cache_data
@st.cache_resource
def get_rag_chain(file_path):
    chat = get_client_of_openai()

    # 1) ì±—ë´‡ì— 'ê¸°ì–µ'ì„ ì…íˆê¸° ìœ„í•œ ì²«ë²ˆì§¸ ë‹¨ê³„
    retriever_of_vectorstore = __get_retriever_of_vectorstore(file_path, chat)

    # 2) ë‘ë²ˆì§¸ ë‹¨ê³„ë¡œ, ë°©ê¸ˆ ì „ ìƒì„±í•œ ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” retriever ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    rag_chain = __get_rag_chain(chat, retriever_of_vectorstore)
    return rag_chain

def __get_retriever_of_vectorstore(file_path, chat):

    vectorstore = __get_vectorstore(file_path)
    retriever = vectorstore.as_retriever(k=2)

    prompt_of_vectorstore = __get_prompt_of_vectorstore()

    # ì´ë¥¼ í† ëŒ€ë¡œ ë©”ì„¸ì§€ ê¸°ë¡ì„ ê¸°ì–µí•˜ëŠ” retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    history_aware_retriever = create_history_aware_retriever(
        chat, retriever, prompt_of_vectorstore
    )
    return history_aware_retriever

def __get_vectorstore(file_path):
    # load data
    loader = PyPDFLoader(
        file_path
    )
    pages = loader.load_and_split()

    # add data to Chroma
    vectorstore = Chroma.from_documents(pages, OpenAIEmbeddings())
    return vectorstore

def __get_prompt_of_vectorstore():
    contextualize_q_system_prompt = """
        ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
        ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""

    # MessagesPlaceholder: 'chat_history' ì…ë ¥ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ë©”ì„¸ì§€ ê¸°ë¡ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚´.
    # ì¦‰ í”„ë¡¬í”„íŠ¸, ë©”ì„¸ì§€ ê¸°ë¡ (ë¬¸ë§¥ ì •ë³´), ì‚¬ìš©ìì˜ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ê°€ êµ¬ì„±ë¨. 
    prompt_of_vectorstore = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    return prompt_of_vectorstore


def __get_rag_chain(chat, history_aware_retriever):
    qa_system_prompt = """
    ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤. 
    ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”. 
    ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. 
    ë‹µë³€ì€ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.

    ## ë‹µë³€ ì˜ˆì‹œ
    ğŸ“ë‹µë³€ ë‚´ìš©: 
    ğŸ“ì¦ê±°: 

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)

    # ê²°ê³¼ê°’ì€ input, chat_history, context, answer í¬í•¨í•¨.
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain






